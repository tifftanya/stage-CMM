import open3d as o3d
import logging as log
import sys
import math
import time
import cv2
import numpy as np
import pyrealsense2 as rs
import trimesh
from time import perf_counter
from argparse import ArgumentParser
from pathlib import Path
from matplotlib import pyplot as plt
import pdb


from instance_segmentation_demo.tracker import StaticIOUTracker

sys.path.append(str(Path(__file__).resolve().parents[2] / 'common/python'))
sys.path.append(str(Path(__file__).resolve().parents[2] / 'common/python/openvino/model_zoo'))

from model_api.models import MaskRCNNModel, YolactModel, OutputTransform
from model_api.adapters import create_core, OpenvinoAdapter, OVMSAdapter
from model_api.pipelines import get_user_config, AsyncPipeline
from model_api.performance_metrics import PerformanceMetrics

import monitors
from images_capture import open_images_capture
from helpers import resolution, log_latency_per_stage
from visualizers import InstanceSegmentationVisualizer

log.basicConfig(format='[ %(levelname)s ] %(message)s', level=log.DEBUG, stream=sys.stdout)


def get_model(model_adapter, configuration):
    inputs = model_adapter.get_input_layers()
    outputs = model_adapter.get_output_layers()
    if len(inputs) == 1 and len(outputs) == 4 and 'proto' in outputs.keys():
        return YolactModel(model_adapter, configuration)
    return MaskRCNNModel(model_adapter, configuration)


def print_raw_results(boxes, classes, scores, frame_id):
    log.debug('  -------------------------- Frame # {} --------------------------  '.format(frame_id))
    log.debug('  Class ID | Confidence |     XMIN |     YMIN |     XMAX |     YMAX ')
    for box, cls, score in zip(boxes, classes, scores):
        log.debug('{:>10} | {:>10f} | {:>8.2f} | {:>8.2f} | {:>8.2f} | {:>8.2f} '.format(cls, score, *box))



def build_argparser():
    parser = ArgumentParser()
    args = parser.add_argument_group('Options')
    args.add_argument('-m', '--model', required=True,
                      help='Required. Path to an .xml file with a trained model '
                           'or address of model inference service if using ovms adapter.')
    args.add_argument('--adapter', default='openvino', choices=('openvino', 'ovms'),
                      help='Optional. Specify the model adapter. Default is openvino.')
    args.add_argument('-i', '--input', required=True,
                      help='Required. An input to process. The input must be a single image, '
                           'a folder of images, video file or camera id.')
    args.add_argument('-d', '--device', default='CPU',
                      help='Optional. Specify the target device to infer on; CPU or GPU is '
                           'acceptable. The demo will look for a suitable plugin for device specified. '
                           'Default value is CPU.')

    common_model_args = parser.add_argument_group('Common model options')
    common_model_args.add_argument('--labels', required=True,
                                   help='Required. Path to a text file with class labels.')
    common_model_args.add_argument('-t', '--prob_threshold', default=0.5, type=float,
                                   help='Optional. Probability threshold for detections filtering.')
    common_model_args.add_argument('--no_track', action='store_true',
                                   help='Optional. Disable object tracking for video/camera input.')
    common_model_args.add_argument('--show_scores', action='store_true',
                                   help='Optional. Show detection scores.')
    common_model_args.add_argument('--show_boxes', action='store_true',
                                   help='Optional. Show bounding boxes.')
    common_model_args.add_argument('--layout', type=str, default=None,
                                   help='Optional. Model inputs layouts. '
                                        'Format "[<layout>]" or "<input1>[<layout1>],<input2>[<layout2>]" in case of more than one input. '
                                        'To define layout you should use only capital letters')

    infer_args = parser.add_argument_group('Inference options')
    infer_args.add_argument('-nireq', '--num_infer_requests', default=0, type=int,
                            help='Optional. Number of infer requests')
    infer_args.add_argument('-nstreams', '--num_streams', default='',
                            help='Optional. Number of streams to use for inference on the CPU or/and GPU in throughput '
                                 'mode (for HETERO and MULTI device cases use format '
                                 '<device1>:<nstreams1>,<device2>:<nstreams2> or just <nstreams>).')
    infer_args.add_argument('-nthreads', '--num_threads', default=None, type=int,
                            help='Optional. Number of threads to use for inference on CPU (including HETERO cases).')

    io_args = parser.add_argument_group('Input/output options')
    io_args.add_argument('--loop', action='store_true',
                         help='Optional. Enable reading the input in a loop.')
    io_args.add_argument('-o', '--output',
                         help='Optional. Name of the output file(s) to save.')
    io_args.add_argument('-limit', '--output_limit', default=1000, type=int,
                         help='Optional. Number of frames to store in output. '
                              'If 0 is set, all frames are stored.')
    io_args.add_argument('--no_show', action='store_true',
                         help="Optional. Don't show output.")
    io_args.add_argument('--output_resolution', default=None, type=resolution,
                         help='Optional. Specify the maximum output window resolution '
                              'in (width x height) format. Example: 1280x720. '
                              'Input frame size used by default.')
    io_args.add_argument('-u', '--utilization_monitors',
                         help='Optional. List of monitors to show initially.')

    debug_args = parser.add_argument_group('Debug options')
    debug_args.add_argument('-r', '--raw_output_message', action='store_true',
                             help='Optional. Output inference results raw values showing.')
    return parser

        

class AppState:

    def __init__(self, *args, **kwargs):
        self.WIN_NAME = 'RealSense'
        self.pitch, self.yaw = math.radians(-10), math.radians(-15)
        self.translation = np.array([0, 0, -1], dtype=np.float32)
        self.distance = 2
        self.prev_mouse = 0, 0
        self.mouse_btns = [False, False, False]
        self.paused = False
        self.decimate = 1
        self.scale = True
        self.color = True

    def reset(self):
        self.pitch, self.yaw, self.distance = 0, 0, 2
        self.translation[:] = 0, 0, -1

    @property
    def rotation(self):
        Rx, _ = cv2.Rodrigues((self.pitch, 0, 0))
        Ry, _ = cv2.Rodrigues((0, self.yaw, 0))
        return np.dot(Ry, Rx).astype(np.float32)

    @property
    def pivot(self):
        return self.translation + np.array((0, 0, self.distance), dtype=np.float32)

args = build_argparser().parse_args()
    
state = AppState()

# Configure depth and color streams
RSpipeline = rs.pipeline()
config = rs.config()

RSpipeline_wrapper = rs.pipeline_wrapper(RSpipeline)
RSpipeline_profile = config.resolve(RSpipeline_wrapper)
device = RSpipeline_profile.get_device()

found_rgb = False
for s in device.sensors:
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True
        break
if not found_rgb:
    print("The demo requires Depth camera with Color sensor")
    exit(0)

config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)

# Start streaming
RSpipeline.start(config)


#if args.adapter == 'openvino':
plugin_config = get_user_config(args.device, args.num_streams, args.num_threads)
model_adapter = OpenvinoAdapter(create_core(), args.model, device=args.device, plugin_config=plugin_config,
                                max_num_requests=args.num_infer_requests,
                                model_parameters={'input_layouts': args.layout})
#elif args.adapter == 'ovms':
#    model_adapter = OVMSAdapter(args.model)

configuration = {
    'confidence_threshold': args.prob_threshold,
    'path_to_labels': args.labels,
}
model = get_model(model_adapter, configuration)
model.log_layers_info()

pipeline = AsyncPipeline(model)
next_frame_id = 0
next_frame_id_to_show = 0

tracker = None
if not args.no_track: # and cap.get_type() in {'VIDEO', 'CAMERA'}:
    tracker = StaticIOUTracker()
visualizer = InstanceSegmentationVisualizer(model.labels, args.show_boxes, args.show_scores)

metrics = PerformanceMetrics()
render_metrics = PerformanceMetrics()
presenter = None
output_transform = None
video_writer = cv2.VideoWriter()
    
# Get stream profile and camera intrinsics
profile = RSpipeline.get_active_profile()
depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))
depth_intrinsics = depth_profile.get_intrinsics()
w, h = depth_intrinsics.width, depth_intrinsics.height

# Processing blocks
pc = rs.pointcloud()
decimate = rs.decimation_filter()
decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)
colorizer = rs.colorizer()

cv2.namedWindow(state.WIN_NAME)#, cv2.WINDOW_AUTOSIZE)
#cv2.resizeWindow(state.WIN_NAME, w, h)

vis = o3d.visualization.Visualizer()
vis.create_window(height=480, width=640)
count = 0 

out = np.empty((h, w, 3), dtype=np.uint8)

# Get new image/frame
start_time = perf_counter()
dt = 1

while True: 
    frames = RSpipeline.wait_for_frames()
    start_time = perf_counter()

    depth_frame = frames.get_depth_frame()
    color_frame = frames.get_color_frame()

    color_image = np.asanyarray(color_frame.get_data())

    pipeline.submit_data(color_image, next_frame_id, {'frame': color_image, 'depth': depth_frame, 'start_time': start_time})
    next_frame_id += 1

    results = pipeline.get_result(next_frame_id_to_show)
    if results:
            (scores, classes, boxes, masks), frame_meta = results
            color_image, depth_frame = frame_meta['frame'], frame_meta['depth']
            depth_image = np.asanyarray(depth_frame.get_data())
            start_time = frame_meta['start_time']

            # mask the background in the rgb image
            for mask in masks:
                bkg_mask = np.dstack((mask,mask,mask))
                color_image = np.multiply( bkg_mask, color_image)
            # mask the background in the depth image
            bkg_mask = np.zeros(depth_image.shape)
            for mask in masks:
                bkg_mask = np.maximum(bkg_mask, mask.astype(float))
            depth_image = np.multiply( bkg_mask, depth_image)

            depth_image_uint16 = (depth_image).astype(np.uint16)

            depth_o3d = o3d.geometry.Image(depth_image_uint16)
            color_o3d = o3d.geometry.Image(color_image)
            rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(color_o3d, depth_o3d, convert_rgb_to_intensity=False)

            intrinsics = rs.video_stream_profile(color_frame.profile).get_intrinsics()
            camera_intrinsic = o3d.camera.PinholeCameraIntrinsic()
            camera_intrinsic.set_intrinsics(
                width=intrinsics.width,
                height=intrinsics.height,
                fx=intrinsics.fx,
                fy=intrinsics.fy,
                cx=intrinsics.ppx,
                cy=intrinsics.ppy
            )

            if "pcd" in globals():
                del pcd 
            pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, camera_intrinsic)  

            #pcd = pcd.uniform_down_sample(30)

            #o3d.visualization.draw_geometries([pcd])

            # outliers removal
            #cl, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=20.0)
            #pcd = pcd.select_by_index(ind)

            # estimate normals
            #pcd.estimate_normals()
            #pcd.orient_normals_to_align_with_direction()

            if count == 0:
                vis.add_geometry(pcd)
                print("adding geometry")
                count += 1 
            else:  
                vis.update_geometry(pcd)  
                print("updating geometry")  
            vis.poll_events()
            vis.update_renderer() 

            # surface reconstruction
            #mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=10, n_threads=1)[0]

            # rotate the mesh
            #rotation = mesh.get_rotation_matrix_from_xyz((np.pi, np.pi, 0))
            #mesh.rotate(rotation, center=(0, 0, 0))

            # save the mesh
            #o3d.io.write_triangle_mesh(f'./cam_mesh.obj', mesh)

            # visualize the mesh
            #o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True) # update geometry function for non blocking rendering 

            if args.raw_output_message:
                print_raw_results(boxes, classes, scores, next_frame_id_to_show)

            rendering_start_time = perf_counter()
            masks_tracks_ids = tracker(masks, classes) if tracker else None
            color_image = visualizer(color_image, boxes, classes, scores, masks, masks_tracks_ids)
            render_metrics.update(rendering_start_time)

            #presenter.drawGraphs(color_image)
            metrics.update(start_time, color_image)

            if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit - 1):
                video_writer.write(color_image)
            next_frame_id_to_show += 1

            if not args.no_show:
                cv2.imshow('Instance Segmentation results', color_image)
                cv2.imshow('depth image', (0.17*np.minimum(depth_image,15000)).astype(np.uint8))
                key = cv2.waitKey(1)
                if key == 27 or key == 113 or key == 225: # end if Esc, q or Q key pressed
                    break
                #presenter.handleKey(key)
vis.destroy_window()
RSpipeline.stop()
