# License: Apache 2.0. See LICENSE file in root directory.
# Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.

"""
OpenCV and Numpy Point cloud Software Renderer

This sample is mostly for demonstration and educational purposes.
It really doesn't offer the quality or performance that can be
achieved with hardware acceleration.

Usage:
------
Mouse: 
    Drag with left button to rotate around pivot (thick small axes), 
    with right button to translate and the wheel to zoom.

Keyboard: 
    [p]     Pause
    [r]     Reset View    
    [d]     Cycle through decimation values
    [z]     Toggle point scaling
    [c]     Toggle color source
    [s]     Save PNG (./out.png)
    [e]     Export points to ply (./out.ply)
    [q\ESC] Quit
"""

import logging as log
import sys
import math
import time
import cv2
import numpy as np
import pyrealsense2 as rs
import trimesh
from time import perf_counter
from argparse import ArgumentParser
from pathlib import Path
from matplotlib import pyplot as plt
import pdb

from instance_segmentation_demo.tracker import StaticIOUTracker

sys.path.append(str(Path(__file__).resolve().parents[2] / 'common/python'))
sys.path.append(str(Path(__file__).resolve().parents[2] / 'common/python/openvino/model_zoo'))

from model_api.models import MaskRCNNModel, YolactModel, OutputTransform
from model_api.adapters import create_core, OpenvinoAdapter, OVMSAdapter
from model_api.pipelines import get_user_config, AsyncPipeline
from model_api.performance_metrics import PerformanceMetrics

import monitors
from images_capture import open_images_capture
from helpers import resolution, log_latency_per_stage
from visualizers import InstanceSegmentationVisualizer

log.basicConfig(format='[ %(levelname)s ] %(message)s', level=log.DEBUG, stream=sys.stdout)


def get_model(model_adapter, configuration):
    inputs = model_adapter.get_input_layers()
    outputs = model_adapter.get_output_layers()
    if len(inputs) == 1 and len(outputs) == 4 and 'proto' in outputs.keys():
        return YolactModel(model_adapter, configuration)
    return MaskRCNNModel(model_adapter, configuration)


def print_raw_results(boxes, classes, scores, frame_id):
    log.debug('  -------------------------- Frame # {} --------------------------  '.format(frame_id))
    log.debug('  Class ID | Confidence |     XMIN |     YMIN |     XMAX |     YMAX ')
    for box, cls, score in zip(boxes, classes, scores):
        log.debug('{:>10} | {:>10f} | {:>8.2f} | {:>8.2f} | {:>8.2f} | {:>8.2f} '.format(cls, score, *box))



def build_argparser():
    parser = ArgumentParser()
    args = parser.add_argument_group('Options')
    args.add_argument('-m', '--model', required=True,
                      help='Required. Path to an .xml file with a trained model '
                           'or address of model inference service if using ovms adapter.')
    args.add_argument('--adapter', default='openvino', choices=('openvino', 'ovms'),
                      help='Optional. Specify the model adapter. Default is openvino.')
    args.add_argument('-i', '--input', required=True,
                      help='Required. An input to process. The input must be a single image, '
                           'a folder of images, video file or camera id.')
    args.add_argument('-d', '--device', default='CPU',
                      help='Optional. Specify the target device to infer on; CPU or GPU is '
                           'acceptable. The demo will look for a suitable plugin for device specified. '
                           'Default value is CPU.')

    common_model_args = parser.add_argument_group('Common model options')
    common_model_args.add_argument('--labels', required=True,
                                   help='Required. Path to a text file with class labels.')
    common_model_args.add_argument('-t', '--prob_threshold', default=0.5, type=float,
                                   help='Optional. Probability threshold for detections filtering.')
    common_model_args.add_argument('--no_track', action='store_true',
                                   help='Optional. Disable object tracking for video/camera input.')
    common_model_args.add_argument('--show_scores', action='store_true',
                                   help='Optional. Show detection scores.')
    common_model_args.add_argument('--show_boxes', action='store_true',
                                   help='Optional. Show bounding boxes.')
    common_model_args.add_argument('--layout', type=str, default=None,
                                   help='Optional. Model inputs layouts. '
                                        'Format "[<layout>]" or "<input1>[<layout1>],<input2>[<layout2>]" in case of more than one input. '
                                        'To define layout you should use only capital letters')

    infer_args = parser.add_argument_group('Inference options')
    infer_args.add_argument('-nireq', '--num_infer_requests', default=0, type=int,
                            help='Optional. Number of infer requests')
    infer_args.add_argument('-nstreams', '--num_streams', default='',
                            help='Optional. Number of streams to use for inference on the CPU or/and GPU in throughput '
                                 'mode (for HETERO and MULTI device cases use format '
                                 '<device1>:<nstreams1>,<device2>:<nstreams2> or just <nstreams>).')
    infer_args.add_argument('-nthreads', '--num_threads', default=None, type=int,
                            help='Optional. Number of threads to use for inference on CPU (including HETERO cases).')

    io_args = parser.add_argument_group('Input/output options')
    io_args.add_argument('--loop', action='store_true',
                         help='Optional. Enable reading the input in a loop.')
    io_args.add_argument('-o', '--output',
                         help='Optional. Name of the output file(s) to save.')
    io_args.add_argument('-limit', '--output_limit', default=1000, type=int,
                         help='Optional. Number of frames to store in output. '
                              'If 0 is set, all frames are stored.')
    io_args.add_argument('--no_show', action='store_true',
                         help="Optional. Don't show output.")
    io_args.add_argument('--output_resolution', default=None, type=resolution,
                         help='Optional. Specify the maximum output window resolution '
                              'in (width x height) format. Example: 1280x720. '
                              'Input frame size used by default.')
    io_args.add_argument('-u', '--utilization_monitors',
                         help='Optional. List of monitors to show initially.')

    debug_args = parser.add_argument_group('Debug options')
    debug_args.add_argument('-r', '--raw_output_message', action='store_true',
                             help='Optional. Output inference results raw values showing.')
    return parser

        

class AppState:

    def __init__(self, *args, **kwargs):
        self.WIN_NAME = 'RealSense'
        self.pitch, self.yaw = math.radians(-10), math.radians(-15)
        self.translation = np.array([0, 0, -1], dtype=np.float32)
        self.distance = 2
        self.prev_mouse = 0, 0
        self.mouse_btns = [False, False, False]
        self.paused = False
        self.decimate = 1
        self.scale = True
        self.color = True

    def reset(self):
        self.pitch, self.yaw, self.distance = 0, 0, 2
        self.translation[:] = 0, 0, -1

    @property
    def rotation(self):
        Rx, _ = cv2.Rodrigues((self.pitch, 0, 0))
        Ry, _ = cv2.Rodrigues((0, self.yaw, 0))
        return np.dot(Ry, Rx).astype(np.float32)

    @property
    def pivot(self):
        return self.translation + np.array((0, 0, self.distance), dtype=np.float32)


def mouse_cb(event, x, y, flags, param):

    if event == cv2.EVENT_LBUTTONDOWN:
        state.mouse_btns[0] = True

    if event == cv2.EVENT_LBUTTONUP:
        state.mouse_btns[0] = False

    if event == cv2.EVENT_RBUTTONDOWN:
        state.mouse_btns[1] = True

    if event == cv2.EVENT_RBUTTONUP:
        state.mouse_btns[1] = False

    if event == cv2.EVENT_MBUTTONDOWN:
        state.mouse_btns[2] = True

    if event == cv2.EVENT_MBUTTONUP:
        state.mouse_btns[2] = False

    if event == cv2.EVENT_MOUSEMOVE:

        h, w = out.shape[:2]
        dx, dy = x - state.prev_mouse[0], y - state.prev_mouse[1]

        if state.mouse_btns[0]:
            state.yaw += float(dx) / w * 2
            state.pitch -= float(dy) / h * 2

        elif state.mouse_btns[1]:
            dp = np.array((dx / w, dy / h, 0), dtype=np.float32)
            state.translation -= np.dot(state.rotation, dp)

        elif state.mouse_btns[2]:
            dz = math.sqrt(dx**2 + dy**2) * math.copysign(0.01, -dy)
            state.translation[2] += dz
            state.distance -= dz

    if event == cv2.EVENT_MOUSEWHEEL:
        dz = math.copysign(0.1, flags)
        state.translation[2] += dz
        state.distance -= dz

    state.prev_mouse = (x, y)



def project(v):
    """project 3d vector array to 2d"""
    h, w = out.shape[:2]
    view_aspect = float(h)/w

    # ignore divide by zero for invalid depth
    with np.errstate(divide='ignore', invalid='ignore'):
        proj = v[:, :-1] / v[:, -1, np.newaxis] * \
            (w*view_aspect, h) + (w/2.0, h/2.0)

    # near clipping
    znear = 0.03
    proj[v[:, 2] < znear] = np.nan
    return proj


def view(v):
    """apply view transformation on vector array"""
    return np.dot(v - state.pivot, state.rotation) + state.pivot - state.translation


def line3d(out, pt1, pt2, color=(0x80, 0x80, 0x80), thickness=1):
    """draw a 3d line from pt1 to pt2"""
    p0 = project(pt1.reshape(-1, 3))[0]
    p1 = project(pt2.reshape(-1, 3))[0]
    if np.isnan(p0).any() or np.isnan(p1).any():
        return
    p0 = tuple(p0.astype(int))
    p1 = tuple(p1.astype(int))
    rect = (0, 0, out.shape[1], out.shape[0])
    inside, p0, p1 = cv2.clipLine(rect, p0, p1)
    if inside:
        cv2.line(out, p0, p1, color, thickness, cv2.LINE_AA)


def grid(out, pos, rotation=np.eye(3), size=1, n=10, color=(0x80, 0x80, 0x80)):
    """draw a grid on xz plane"""
    pos = np.array(pos)
    s = size / float(n)
    s2 = 0.5 * size
    for i in range(0, n+1):
        x = -s2 + i*s
        line3d(out, view(pos + np.dot((x, 0, -s2), rotation)),
               view(pos + np.dot((x, 0, s2), rotation)), color)
    for i in range(0, n+1):
        z = -s2 + i*s
        line3d(out, view(pos + np.dot((-s2, 0, z), rotation)),
               view(pos + np.dot((s2, 0, z), rotation)), color)


def axes(out, pos, rotation=np.eye(3), size=0.075, thickness=2):
    """draw 3d axes"""
    line3d(out, pos, pos +
           np.dot((0, 0, size), rotation), (0xff, 0, 0), thickness)
    line3d(out, pos, pos +
           np.dot((0, size, 0), rotation), (0, 0xff, 0), thickness)
    line3d(out, pos, pos +
           np.dot((size, 0, 0), rotation), (0, 0, 0xff), thickness)


def frustum(out, intrinsics, color=(0x40, 0x40, 0x40)):
    """draw camera's frustum"""
    orig = view([0, 0, 0])
    w, h = intrinsics.width, intrinsics.height

    for d in range(1, 6, 2):
        def get_point(x, y):
            p = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], d)
            line3d(out, orig, view(p), color)
            return p

        top_left = get_point(0, 0)
        top_right = get_point(w, 0)
        bottom_right = get_point(w, h)
        bottom_left = get_point(0, h)

        line3d(out, view(top_left), view(top_right), color)
        line3d(out, view(top_right), view(bottom_right), color)
        line3d(out, view(bottom_right), view(bottom_left), color)
        line3d(out, view(bottom_left), view(top_left), color)


def pointcloud(out, verts, texcoords, color, painter=True):
    """draw point cloud with optional painter's algorithm"""
    if painter:
        # Painter's algo, sort points from back to front

        # get reverse sorted indices by z (in view-space)
        # https://gist.github.com/stevenvo/e3dad127598842459b68
        v = view(verts)
        s = v[:, 2].argsort()[::-1]
        proj = project(v[s])
    else:
        proj = project(view(verts))

    if state.scale:
        proj *= 0.5**state.decimate

    h, w = out.shape[:2]

    # proj now contains 2d image coordinates
    j, i = proj.astype(np.uint32).T

    # create a mask to ignore out-of-bound indices
    im = (i >= 0) & (i < h)
    jm = (j >= 0) & (j < w)
    m = im & jm

    cw, ch = color.shape[:2][::-1]
    if painter:
        # sort texcoord with same indices as above
        # texcoords are [0..1] and relative to top-left pixel corner,
        # multiply by size and add 0.5 to center
        v, u = (texcoords[s] * (cw, ch) + 0.5).astype(np.uint32).T
    else:
        v, u = (texcoords * (cw, ch) + 0.5).astype(np.uint32).T
    # clip texcoords to image
    np.clip(u, 0, ch-1, out=u)
    np.clip(v, 0, cw-1, out=v)

    # perform uv-mapping
    out[i[m], j[m]] = color[u[m], v[m]]





# ================================================================
# ================================================================
    
args = build_argparser().parse_args()
    
state = AppState()

# Configure depth and color streams
RSpipeline = rs.pipeline()
config = rs.config()

RSpipeline_wrapper = rs.pipeline_wrapper(RSpipeline)
RSpipeline_profile = config.resolve(RSpipeline_wrapper)
device = RSpipeline_profile.get_device()

found_rgb = False
for s in device.sensors:
    if s.get_info(rs.camera_info.name) == 'RGB Camera':
        found_rgb = True
        break
if not found_rgb:
    print("The demo requires Depth camera with Color sensor")
    exit(0)

config.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
config.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)

# Start streaming
RSpipeline.start(config)

#if args.adapter == 'openvino':
plugin_config = get_user_config(args.device, args.num_streams, args.num_threads)
model_adapter = OpenvinoAdapter(create_core(), args.model, device=args.device, plugin_config=plugin_config,
                                max_num_requests=args.num_infer_requests,
                                model_parameters={'input_layouts': args.layout})
#elif args.adapter == 'ovms':
#    model_adapter = OVMSAdapter(args.model)

configuration = {
    'confidence_threshold': args.prob_threshold,
    'path_to_labels': args.labels,
}
model = get_model(model_adapter, configuration)
model.log_layers_info()

pipeline = AsyncPipeline(model)
next_frame_id = 0
next_frame_id_to_show = 0

tracker = None
if not args.no_track: # and cap.get_type() in {'VIDEO', 'CAMERA'}:
    tracker = StaticIOUTracker()
visualizer = InstanceSegmentationVisualizer(model.labels, args.show_boxes, args.show_scores)

metrics = PerformanceMetrics()
render_metrics = PerformanceMetrics()
presenter = None
output_transform = None
video_writer = cv2.VideoWriter()
    
# Get stream profile and camera intrinsics
profile = RSpipeline.get_active_profile()
depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))
depth_intrinsics = depth_profile.get_intrinsics()
w, h = depth_intrinsics.width, depth_intrinsics.height

# Processing blocks
pc = rs.pointcloud()
decimate = rs.decimation_filter()
decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)
colorizer = rs.colorizer()

cv2.namedWindow(state.WIN_NAME)#, cv2.WINDOW_AUTOSIZE)
#cv2.resizeWindow(state.WIN_NAME, w, h)
cv2.setMouseCallback(state.WIN_NAME, mouse_cb)

out = np.empty((h, w, 3), dtype=np.uint8)

# Get new image/frame
start_time = perf_counter()
dt = 1

from numpy import linalg as LA
from itertools import combinations
from scipy.spatial import Delaunay
import skimage

def triangulate_mask (depth):
    tin = time.time()
    V = depth.transpose()>0
    points = np.array(np.nonzero(V)).transpose()
    tri = Delaunay(points)
    new_simplices = np.empty((0,3), dtype=np.int32)
    LL = np.zeros(3)
    for smpl in tri.simplices:
        ii=0
        for edge in combinations(smpl,2):
            LL[ii]= LA.norm(np.vstack((tri.points[edge[0]]-tri.points[edge[1]])))
            ii += 1
        if np.max(LL)<1.5:
            new_simplices = np.vstack((new_simplices, smpl))
    tri.simplices = new_simplices
    print (time.time()-tin)
    return tri

def face_colors(mesh):
    """
    Colors defined for each face of a mesh.
    """
    return mesh._get_colors(name='face')



while True:
    # Grab camera data
    if not state.paused:
        # Wait for a coherent pair of frames: depth and color
        frames = RSpipeline.wait_for_frames()
        start_time = perf_counter()
        
        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        
        color_image = np.asanyarray(color_frame.get_data())
        
        if next_frame_id == 0:
            output_transform = OutputTransform(color_image.shape[:2], args.output_resolution)
            if args.output_resolution:
                output_resolution = output_transform.new_resolution
            else:
                output_resolution = (color_image.shape[1], color_image.shape[0])
            presenter = monitors.Presenter(args.utilization_monitors, 55,
                                           (round(output_resolution[0] / 4), round(output_resolution[1] / 8)))
            if args.output and not video_writer.open(args.output, cv2.VideoWriter_fourcc(*'MJPG'),
                                                     10, tuple(output_resolution)):
                raise RuntimeError("Can't open video writer")
        # Submit for inference
        pipeline.submit_data(color_image, next_frame_id, {'frame': color_image, 'depth': depth_frame, 'start_time': start_time})
        next_frame_id += 1

        if pipeline.callback_exceptions:
            raise pipeline.callback_exceptions[0]
        
        # Process all completed requests
        results = pipeline.get_result(next_frame_id_to_show)
        if results:
            (scores, classes, boxes, masks), frame_meta = results
            color_image, depth_frame = frame_meta['frame'], frame_meta['depth']
            depth_image = np.asanyarray(depth_frame.get_data())
            start_time = frame_meta['start_time']

            # mask the background in the rgb image
            for mask in masks:
                bkg_mask = np.dstack((mask,mask,mask))
                color_image = np.multiply( bkg_mask, color_image)
            # mask the background in the depth image
            bkg_mask = np.zeros(depth_image.shape)
            for mask in masks:
                bkg_mask = np.maximum(bkg_mask, mask.astype(float))
            depth_image = np.multiply( bkg_mask, depth_image)
            
            depth_image = depth_image.astype(np.uint16) 
            
            # added 
            import matplotlib.pyplot as plt
            from mpl_toolkits.mplot3d import Axes3D
            from scipy.spatial import Delaunay, ConvexHull
            from alphashape import alphashape

            segmented_depth = depth_image * bkg_mask
            try:
                depth = skimage.transform.rescale (segmented_depth, .1)
                print(segmented_depth.shape)
                print(depth.shape)

                tri = triangulate_mask (depth)

                #points = tri.points
                plt.figure(1)
                plt.clf()
                plt.triplot(tri.points[:,0], tri.points[:,1], tri.simplices)
                plt.plot(tri.points[:,0], tri.points[:,1], 'o')
                plt.gca().invert_yaxis()
                plt.show(block=False)


                xx,yy = tri.points.astype(np.int32).transpose()
                zz = -depth[yy,xx]
                pts = np.vstack((xx,yy,zz)).transpose()


                # Compute alpha shape
                hull = ConvexHull(pts)
                alpha = 1.0  # Adjust alpha value as needed
                edges = alphashape(pts, alpha=alpha).edges


                # Retrieve face indices triangle de la surface 
                faces = tri.simplices

                # Create trimesh object
                mesh = trimesh.Trimesh(vertices=tri.points, faces=faces)

                # Plot the mesh
                fig = plt.figure(3)
                ax = fig.add_subplot(111, projection='3d')

                # Plot mesh by connecting points directly
                for simplex in tri.simplices:
                    x = tri.points[simplex, 0]
                    y = tri.points[simplex, 1]
                    z = zz[simplex]
                    ax.plot(x, y, z, color='yellow')

                plt.show(block=False)

                # # Export mesh as OBJ
                # output_file = "mesh.obj"
                # mesh.export(output_file)



                # plt.figure(3)
                # ax = plt.axes(projection='3d')

                # # Plot mesh by connecting points directly
                # for simplex in tri.simplices:
                #     x = tri.points[simplex, 0]
                #     y = tri.points[simplex, 1]
                #     z = zz[simplex]
                #     ax.plot(x, y, z, color='yellow')

                # ax.view_init(elev=-72, azim=-105, roll=15)

                # plt.show(block=True)

                
                # # Create Open3D mesh
                # # Create trimesh object
                # vertices=tri.points
                # faces=tri.simplices
                # mesh = trimesh.Trimesh(vertices=tri.points, faces=tri.simplices)
                # print("Vertices (tri.points):")
                # print(tri.points)

                # print("Faces (tri.simplices): {tri.simplices}")


                # output_file = "mesh.obj"
                # mesh.export(output_file, faces=faces)



                # Retrieve face colors from the mesh
                #colors = face_colors(mesh)

                # Generate vertex colors from color_image
            #     colors = []
            #     for vertex in tri.points:
            #         x, y = int(vertex[0]), int(vertex[1])
            #         colors.append(color_image[y, x])
            #     colors = np.array(colors, dtype=np.uint8)

            #     # Apply vertex colors to mesh
            #     mesh.visual = trimesh.visual.color.ColorVisuals(
            #         vertex_colors=colors,
            #         face_colors=trimesh.visual.random_color(mesh.faces.shape[0])
            #     )

            #     # Convert trimesh to Open3D geometry
            #     vertices = np.asarray(mesh.vertices)
            #     faces = np.asarray(mesh.faces)
            #     colors = np.asarray(mesh.visual.vertex_colors)

            #     o3d_mesh = o3d.geometry.TriangleMesh()
            #     o3d_mesh.vertices = o3d.utility.Vector3dVector(vertices)
            #     o3d_mesh.triangles = o3d.utility.Vector3iVector(faces)
            #     o3d_mesh.vertex_colors = o3d.utility.Vector3dVector(colors / 255.0)  # Normalize colors to [0, 1]

            #     # Visualize the mesh using Open3D
            #     o3d.visualization.draw_geometries([o3d_mesh])

            #     plt.show(block=True)
            except:
                pass

                # fig = plt.figure(3)
                # ax = fig.add_subplot(111, projection='3d')
                # ax.plot_trisurf(xx, yy, zz, cmap='viridis', edgecolor='none')
                # ax.view_init(elev=-72, azim=-105, roll=15)

                # # Remove yellow lines (edges of alpha shape)
                # for edge in edges:
                #     x = [xx[edge[0]], xx[edge[1]]]
                #     y = [yy[edge[0]], yy[edge[1]]]
                #     z = [zz[edge[0]], zz[edge[1]]]
                #     ax.plot(x, y, z, color='black')

                # ax.xaxis.set_visible(False)
                # ax.yaxis.set_visible(False)
                # ax.zaxis.set_visible(False)



            if args.raw_output_message:
                print_raw_results(boxes, classes, scores, next_frame_id_to_show)

            rendering_start_time = perf_counter()
            masks_tracks_ids = tracker(masks, classes) if tracker else None
            color_image = visualizer(color_image, boxes, classes, scores, masks, masks_tracks_ids)
            render_metrics.update(rendering_start_time)

            presenter.drawGraphs(color_image)
            metrics.update(start_time, color_image)

            if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit - 1):
                video_writer.write(color_image)
            next_frame_id_to_show += 1

            if not args.no_show:
                cv2.imshow('Instance Segmentation results', color_image)
                cv2.imshow('depth image', (0.17*np.minimum(depth_image,15000)).astype(np.uint8))
                key = cv2.waitKey(1)
                if key == 27 or key == 113 or key == 225: # end if Esc, q or Q key pressed
                    break
                presenter.handleKey(key)
            
            depth_frame = decimate.process(depth_frame)
        
            # Grab new intrinsics (may be changed by decimation)
            depth_intrinsics = rs.video_stream_profile(depth_frame.profile).get_intrinsics()
            w, h = depth_intrinsics.width, depth_intrinsics.height

            depth_colormap = np.asanyarray(colorizer.colorize(depth_frame).get_data())
        
            if state.color:
                mapped_frame, color_source = color_frame, color_image
            else:
                mapped_frame, color_source = depth_frame, depth_colormap

            if 'bkg_mask' in locals():
                color_source = np.multiply(color_source, np.dstack((bkg_mask, bkg_mask, bkg_mask))).astype(np.uint8)
        
            points = pc.calculate(depth_frame)
            pc.map_to(mapped_frame)

            # Pointcloud data to arrays
            v, t = points.get_vertices(), points.get_texture_coordinates()
            verts = np.asanyarray(v).view(np.float32).reshape(-1, 3)  # xyz
            texcoords = np.asanyarray(t).view(np.float32).reshape(-1, 2)  # uv

            # Render
            now = time.time()

            out.fill(0)

            grid(out, (0, 0.5, 1), size=1, n=10)
            frustum(out, depth_intrinsics)
            axes(out, view([0, 0, 0]), state.rotation, size=0.1, thickness=1)
    
            if not state.scale or out.shape[:2] == (h, w):
                pointcloud(out, verts, texcoords, color_source)
            else:
                tmp = np.zeros((h, w, 3), dtype=np.uint8)
                pointcloud(tmp, verts, texcoords, color_source)
                tmp = cv2.resize(
                    tmp, out.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)
                np.putmask(out, tmp > 0, tmp)

            if any(state.mouse_btns):
                axes(out, view(state.pivot), state.rotation, thickness=4)

            dt = time.time() - now

            cv2.setWindowTitle(
                state.WIN_NAME, "RealSense (%dx%d) %dFPS (%.2fms) %s" %
                (1280, 720, 1.0/dt, dt*1000, "PAUSED" if state.paused else ""))

            cv2.imshow(state.WIN_NAME, out)
            key = cv2.waitKey(1)
        
            if key == ord("r"):
                state.reset()

            if key == ord("p"):
                state.paused ^= True

            if key == ord("d"):
                state.decimate = (state.decimate + 1) % 3
                decimate.set_option(rs.option.filter_magnitude, 2 ** state.decimate)

            if key == ord("z"):
                state.scale ^= True

            if key == ord("c"):
                state.color ^= True

            if key == ord("s"):
                cv2.imwrite('./out.png', out)

            if key == ord("e"):
                points.export_to_ply('./out.ply', mapped_frame)

            if key in (27, ord("q")) or cv2.getWindowProperty(state.WIN_NAME, cv2.WND_PROP_AUTOSIZE) < 0:
                break
    
    
# Stop streaming
RSpipeline.stop()
